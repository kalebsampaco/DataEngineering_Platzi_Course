{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Arquitecturas de redes convoulcionales.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalebsampaco/DataEngineering_Platzi_Course/blob/master/Copia_de_Arquitecturas_de_redes_convoulcionales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zy7j04WTP4z"
      },
      "source": [
        "# Guia de arquitecturas para redes convolucionales\n",
        "\n",
        "En este tutorial me concentrare en cinco de las arquitectura más populares que he encontrado, estas son:\n",
        "\n",
        "1. VGG16/19\n",
        "\n",
        "2. GoogLeNet/Inception\n",
        "\n",
        "3. Xception\n",
        "\n",
        "4. Mobile net\n",
        "\n",
        "5. Resnet\n",
        "\n",
        "Nota: En esta ocasión solo dejare los snippets de codigo para ilustrar en codigo las arquitecturas de las redes, principalmente porque realizar los ajustes de una sola red es un trabajo exahustivo, no obstante con la base de este notebook cualquiera que lo desee puede hacer los ajustes que desee desde cero. \n",
        "\n",
        "----\n",
        "## Introducción a las redes convolucionales\n",
        "\n",
        "En anteriores notebooks he tocado el tema de las redes convolucionales con más detalle, no obstante leer y escribir más sobre el tema es un gran ejercicio para afianzar conceptos.\n",
        "\n",
        "Dentro del marco de las redes neuronales, las redes convolucionales se ubican como una de las principales herramientas para el reconocimiento de imagenes, detección de objetos, reconocimento de caras, etc. \n",
        "\n",
        "Los computadores interpretan una imagen como un arreglo matricial de pixeles. Basada en la resolución de la imagen se estima el tamaño de la matriz, si es una imagen de más de un canal (Imagenes RGB) se hace nesesario usar tensores para procesarlas. \n",
        "\n",
        "![tensor](https://miro.medium.com/max/289/1*CBY94wikMUCZMB4-Xxs-pw.png)\n",
        "\n",
        "Técnicamente los modelos profundos CNN entrenan y prueban cada imagen al pasarla procesarla por capas convolucionales con filtros(kernels), operaciones de pooling y redes profundas conectadas.\n",
        "\n",
        "![](https://miro.medium.com/max/875/1*XbuW8WuRrAY5pC4t-9DZAQ.jpeg)\n",
        "\n",
        "### la capa convolucional\n",
        "\n",
        "La convolución es la primera capa de la red, su función es la de extraer las caracteristicas de una imagen. La concolución a diferencia de otras técnicas conocidas preserva la relación entre pixeles al aprender las caracteristicas de una imagen usando pequeñas areas de la imagen de entrada, es una operación matematica que toma dos entradas y un filtro o kernel.\n",
        "\n",
        "![](https://miro.medium.com/max/576/1*kYSsNpy0b3fIonQya66VSQ.png)\n",
        "\n",
        "Considerando una imagen de 5x5 cuyos valores de pixeles varian entre 0 y 1 y una matriz filtro de 3x3. La convolución de estas dos matrices consiste en multiplicar la matriz 5x5 con el filtro 3x3. El resultado es lo que se conoce como mapa de caracteristicas.\n",
        "\n",
        "![](https://miro.medium.com/max/335/1*MrGSULUtkXc0Ou07QouV8A.gif)\n",
        "\n",
        "Un proceso de convolucióón con varios filtros puede realizar detección de bordes, opacamiento de una imagen, suviasado de caracteristicas abstractas, entre otras.\n",
        "\n",
        "#### strides\n",
        "\n",
        "Existen varios parametros para definir una red convolucional, el primero es el stride. Este es numero de pixeles en que recorre el filtro dentro de la imagen. Cuando este valor es 1, el filtro cada pixel el filtro avanza dentro de la imagen, cuando es dos, este avanza dos pixeles. La siguiente imagen es una operación de convolución con un stride de dos.\n",
        "\n",
        "![](https://miro.medium.com/max/869/1*nGHLq1hx0gt02OK4l8WmRg.png)\n",
        "\n",
        "#### padding\n",
        "\n",
        "Este es el segundo parametroa definir en una red convolucional. En la mayoria de los casos los filtros no logran recorrer todos los pixeles de una imagen. para esto se aplica un padding, un concepto similar al desarrollo frontend, a la imagen de valores, estos varian entre 0 y 1.\n",
        "\n",
        "### Pooling\n",
        "\n",
        "La operación de pooling es la segunda parte de una red convolucional, su funcióón es la de reducir la dimensionalidad de los parametros, esto debido al gran tamaño de información contenido dentro de una imagen y las limitaciones técnicas generales de procesamiento en la actualidad. La operación consiste en tomar muestras de la imagen y dependiendo de la operación conservar los valores más relevantes. Las operaciones de pooling más conocidas son:\n",
        "\n",
        "* Max Pooling: Conserva el mayor valor de la muestra, es el más usado.\n",
        "\n",
        "* Average Pooling: Conserva el promedio de los valores de la muestra.\n",
        "\n",
        "* Sum Pooling: Suma los valores de la muestra.\n",
        "\n",
        "![](https://miro.medium.com/max/753/1*SmiydxM5lbTjoKWYPiuzWQ.png)\n",
        "\n",
        "### Red conectada profunda\n",
        "\n",
        "Esta es la ultima parte de una red convolucional, como input toma los valores de la imagen ya procesados y los aplana en un vector.\n",
        "\n",
        "![](https://miro.medium.com/max/693/1*Mw6LKUG8AWQhG73H1caT8w.png)\n",
        "\n",
        "Un modelo tipico de red convolucional sería el siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CaaZC2rSOkJ"
      },
      "source": [
        "#Reference : https://www.kaggle.com/shahules/getting-started-with-cnn-and-vgg16\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32,(3,3),activation='relu',input_shape=(150,150,3)))\n",
        "model.add(layers.MaxPool2D((2,2)))\n",
        "model.add(layers.Conv2D(64,(3,3),activation='relu',input_shape=(150,150,3)))\n",
        "model.add(layers.MaxPool2D((2,2)))\n",
        "model.add(layers.Conv2D(128,(3,3),activation='relu',input_shape=(150,150,3)))\n",
        "model.add(layers.MaxPool2D((2,2)))\n",
        "model.add(layers.Conv2D(128,(3,3),activation='relu',input_shape=(150,150,3)))\n",
        "model.add(layers.MaxPool2D((2,2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512,activation='relu'))\n",
        "model.add(layers.Dense(1,activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bh6_BVGeNyb"
      },
      "source": [
        "Al compilar el modelo es nesesario tener en cuenta la definición correcta de la función de perdida, el optimizador y la metrica de evalución.\n",
        "\n",
        "Para el caso del snippet de condigo escrito arriba se podria usar como función de perdida Binary Cross Entropy, como optimizdor Adam y como metrica accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjb44Jp4eGXg"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer=optimizers.adam(),metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOmIcmnJfF6c"
      },
      "source": [
        "Para esta arquitectura se puede hacer un entrenamiento de cinco epocas para obtener un presición del 96.7 %"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyhtsuAffS7f"
      },
      "source": [
        "epochs=5\n",
        "history=model.fit_generator(train_generator,steps_per_epoch=100,epochs=5,validation_data=validation_generator,validation_steps=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VTwi8GmfV_-"
      },
      "source": [
        "Con el siguiente snippet se realiza la evalución del modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcxiqnf5flEd"
      },
      "source": [
        "acc=history.history['acc'] \n",
        "epochs_=range(0,epochs)    \n",
        "plt.plot(epochs_,acc,label='training accuracy')\n",
        "plt.xlabel('no of epochs')\n",
        "plt.ylabel('accuracy')\n",
        "\n",
        "acc_val=history.history['val_acc']  \n",
        "plt.scatter(epochs_,acc_val,label=\"validation accuracy\")\n",
        "plt.title(\"no of epochs vs accuracy\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_pgqb2OfrHS"
      },
      "source": [
        "![](https://www.kaggleusercontent.com/kf/16679264/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..zJeYgm3KlWgVxWherBudEw.G-7EjJ6IAoynRIEQYDeEi-mU0mQuUs7TDRR4lnpasnD7zyf-0RMUBIHoND96yXBQAAoTKBeuMj8_RHtWxJyJvwxssFKMmOyS6TBi4vs3pzvMGWqscAWOz2N9GEhOLcNvampUb2w26qQV8pXTT1zXyMq3sLu_jbW1yaPOWPK9u9MGJc7QR-TURW89ZvNwtPhsWZHxr3wlt-LgbRgBxPRame-_RoqD3s_mQQ3UFx5GEwr1zcI3RVV9How_eJN8DAdanIgN_oW5plV__fYiCFAG9caztpoCbWIUUg4I3DWhzKTALacp5Mfbbv1DaZq-Dymxv_-BqCgtQsxYwE-7_esBo6i-vDUvnxKlVXssmmnY5UxLNkRznFbwCm40HbvfpzxphlDzybh1JnMQDabU1hIEwA4mfrm4FB0Xu7FDWsGT04pR4gygmEo1WKoQ-2wr3TQdOMXWT501WVl13cO85pzUJLXnDZIFub6VEU6qPKdeSc7TlOFsxJXjZWQLd4SNX2310wUR8TzJmtolauzlga0_oGGMTlNkTkc6aTG6kGK23ckLa76LlzQ_gpjFtrWVqjUJR-dWDtfNKZJ9YPutkLyOMK7CIGy6Jyub7BcTkJPGKuwxFr3gfvBBjWctAtqPUmm7Telh0kjzupOVcaUeWf59StuPDwljhPv2khWG-abasgr3n8LUHGMH6MDpH8asXntT.uTnDpuCwwehNhqvXCDJaUg/__results___files/__results___26_1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4SGwySLf0M9"
      },
      "source": [
        "## VGG16\n",
        "\n",
        "Esta fue una de las primeras arquitecturas de redes convolucionales, fue un modelo propuesto por K. Simonyan y A. Zisserman de la universidad de Oxford en su paper “Very Deep Convolutional Networks for Large-Scale Image Recognition”. El modelo logra una presición del 92.7% del dataset ImageNet, el cual consiste en 14 millones de imagenes divididas en 1000 clases diferentes.\n",
        "\n",
        "VGG16 fue entrenado por semanas usando el GPU Titan Black de Nvidia.\n",
        "\n",
        "Esta arquitectura fue una mejora sobre otra llamada AlexNet, cambiando el tamaño de los kernels de 11x11 y 5x5 a un solo tamaño de 3x3, considerado hoy día el default en al gran mayoria de los modelos.\n",
        "\n",
        "Esta es una representacióón gráfica de la arquitectura\n",
        "\n",
        "![](https://neurohive.io/wp-content/uploads/2018/11/vgg16-neural-network.jpg)\n",
        "\n",
        "Hoy en día esta arquitectura tiene diferentes configuraciones:\n",
        "\n",
        "![](https://neurohive.io/wp-content/uploads/2018/11/Capture-564x570.jpg)\n",
        "\n",
        "El tamaño de entrada de las imagenes esta fijo en 224 x 224, todas las configuraciones siguen el diseño generico presentado en la arquitectura original y difieren solo en la profundidad.\n",
        "\n",
        "Esta arquitectura tiene los siguientes contras:\n",
        "\n",
        "* Es muy lenta al entrenar\n",
        "\n",
        "* Los pesos de las variables de entrada de la red son bastante grandes\n",
        "\n",
        "TensorFlow ya tiene precargado este modelo y se puede usar con el siguiente snippet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RFjsmpXisK9"
      },
      "source": [
        "model = applications.VGG16(weights = \"imagenet\", include_top=False, input_shape = (150, 150, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfHooq_piyrH"
      },
      "source": [
        "Es posible sin emabrgo customizar un poco la red para nesecidades espscificas de la siguiente forma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGEmyvtBjDRU"
      },
      "source": [
        "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
        "for layer in model.layers[:5]:\n",
        "    layer.trainable = False\n",
        "\n",
        "#Agregando capas especificas\n",
        "x = model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(1024, activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1024, activation=\"relu\")(x)\n",
        "predictions = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "# creando el modelo final\n",
        "model_final =  Model(inputs=model.input, outputs=predictions)\n",
        "\n",
        "model_final.compile(loss='binary_crossentropy',optimizer=optimizers.adam(),metrics=['acc'])\n",
        "\n",
        "history=model_final.fit_generator(train_generator,steps_per_epoch=100,epochs=5,validation_data=validation_generator,validation_steps=50)\n",
        "\n",
        "acc=history.history['acc']  \n",
        "epochs_=range(0,epochs)    \n",
        "plt.plot(epochs_,acc,label='training accuracy')\n",
        "plt.xlabel('no of epochs')\n",
        "plt.ylabel('accuracy')\n",
        "\n",
        "acc_val=history.history['val_acc']  \n",
        "plt.scatter(epochs_,acc_val,label=\"validation accuracy\")\n",
        "plt.title(\"no of epochs vs accuracy\")\n",
        "plt.legend()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klD-5Wb2jKxT"
      },
      "source": [
        "![](https://www.kaggleusercontent.com/kf/16679264/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..zJeYgm3KlWgVxWherBudEw.G-7EjJ6IAoynRIEQYDeEi-mU0mQuUs7TDRR4lnpasnD7zyf-0RMUBIHoND96yXBQAAoTKBeuMj8_RHtWxJyJvwxssFKMmOyS6TBi4vs3pzvMGWqscAWOz2N9GEhOLcNvampUb2w26qQV8pXTT1zXyMq3sLu_jbW1yaPOWPK9u9MGJc7QR-TURW89ZvNwtPhsWZHxr3wlt-LgbRgBxPRame-_RoqD3s_mQQ3UFx5GEwr1zcI3RVV9How_eJN8DAdanIgN_oW5plV__fYiCFAG9caztpoCbWIUUg4I3DWhzKTALacp5Mfbbv1DaZq-Dymxv_-BqCgtQsxYwE-7_esBo6i-vDUvnxKlVXssmmnY5UxLNkRznFbwCm40HbvfpzxphlDzybh1JnMQDabU1hIEwA4mfrm4FB0Xu7FDWsGT04pR4gygmEo1WKoQ-2wr3TQdOMXWT501WVl13cO85pzUJLXnDZIFub6VEU6qPKdeSc7TlOFsxJXjZWQLd4SNX2310wUR8TzJmtolauzlga0_oGGMTlNkTkc6aTG6kGK23ckLa76LlzQ_gpjFtrWVqjUJR-dWDtfNKZJ9YPutkLyOMK7CIGy6Jyub7BcTkJPGKuwxFr3gfvBBjWctAtqPUmm7Telh0kjzupOVcaUeWf59StuPDwljhPv2khWG-abasgr3n8LUHGMH6MDpH8asXntT.uTnDpuCwwehNhqvXCDJaUg/__results___files/__results___38_1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzmfXOhNjh9Z"
      },
      "source": [
        "## GoogLeNet/Inception\n",
        "\n",
        "Google buscando mejorar la presición y velocidad de las redes realizo un complejo proceso de ingeniería que obtuvo como resultado esta red. \n",
        "\n",
        "El problema a solucionar era que el target que se deseaba identificar dentro de una imagen podia tener diferentes tamaños dentro del marco de la imagen, esto complica la selección del tamaño del kernel apropiado y demora el proceso de entrenamiento de una red convolucional, un ejemplo de la siguiente situción son estas imagenes:\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*aBdPBGAeta-_AM4aEyqeTQ.jpeg)\n",
        "\n",
        "Un kernel de gran tamaño es preferido para imagenes que ocupan un contexto más global dentro de una imagen, mientras que un kernel pequeño es más usado en imagenes que distribuyen de forma local la información. Ademas es conocido que redes muy profundas son sensibles a sobreajuste.\n",
        "\n",
        "Bajo este marco nace InceptionNet, en la actualidad maneja diferentes versiones:\n",
        "\n",
        "* Inception V1\n",
        "\n",
        "* Inception V2 y V3\n",
        "\n",
        "* Inception V4 y Inception-ResNet\n",
        "\n",
        "Cada versión es una mejora iterativa sobre la anterior.\n",
        "\n",
        "La arquitectura consiste en filtros de diferente tamaño que oepran al mismo nivel, se intercambia profundidad por ancho dicho en las mismas palabras de sus creadores, esto se logra a través del modulo inception. En la siguiente imagen se muestra la versión naive del modulo inception:\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*DKjGRDd_lJeUfVlY50ojOA.png)\n",
        "\n",
        "Este cuenta con tres tamaños de filtros 1x1, 3x3 y 5x5. Esta aqruitectura realiza Max Pooling de los resultados de cada filtro y los concatena como salida.\n",
        "\n",
        "Una solucióón hallada por los autores de esta red fue la de incluir una operación de convolcuión 1x1 para reducir la profundidad de la red, esta operación se realiza despues del MaxPooling.\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*U_McJnp7Fnif-lw9iIC5Bw.png)\n",
        "\n",
        "Los modulos inception se juntan uno sobre otro creando la InceptionNet, similar a los modulos residuales usados en las redes CycleGan.\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*uW81y16b-ptBDV8SIT1beQ.png)\n",
        "\n",
        "Si quieres saber más sobre el tema, puedes consultar este link:\n",
        "\n",
        "https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4012Nx9tjekH"
      },
      "source": [
        "inception_model = applications.inception_v3.InceptionV3(weights = \"imagenet\", include_top=False, input_shape = (150, 150, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGB4fDcNo-F6"
      },
      "source": [
        "De igual forma que con VGG16, TensorFlow ya tiene cargada esta arquitectura para uso. El siguiente snippet de codigo puede ser usado para customizar la red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lts9dgiLpYHz"
      },
      "source": [
        "for layer in inception_model.layers[:5]:\n",
        "    layer.trainable = False\n",
        "\n",
        "#Agregando capas personalizadas\n",
        "x = inception_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1024, activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "# creando el modelo final\n",
        "model_final =  Model(inputs=inception_model.input, outputs=predictions)\n",
        "\n",
        "model_final.compile(loss='binary_crossentropy',optimizer=optimizers.adam(),metrics=['acc'])\n",
        "\n",
        "history=model_final.fit_generator(train_generator,steps_per_epoch=100,epochs=5,validation_data=validation_generator,validation_steps=50)\n",
        "\n",
        "acc=history.history['acc']  \n",
        "epochs_=range(0,epochs)    \n",
        "plt.plot(epochs_,acc,label='training accuracy')\n",
        "plt.xlabel('no of epochs')\n",
        "plt.ylabel('accuracy')\n",
        "\n",
        "acc_val=history.history['val_acc']  \n",
        "plt.scatter(epochs_,acc_val,label=\"validation accuracy\")\n",
        "plt.title(\"no of epochs vs accuracy\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAtx9rb6p3XX"
      },
      "source": [
        "![](https://www.kaggleusercontent.com/kf/16679264/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..zJeYgm3KlWgVxWherBudEw.G-7EjJ6IAoynRIEQYDeEi-mU0mQuUs7TDRR4lnpasnD7zyf-0RMUBIHoND96yXBQAAoTKBeuMj8_RHtWxJyJvwxssFKMmOyS6TBi4vs3pzvMGWqscAWOz2N9GEhOLcNvampUb2w26qQV8pXTT1zXyMq3sLu_jbW1yaPOWPK9u9MGJc7QR-TURW89ZvNwtPhsWZHxr3wlt-LgbRgBxPRame-_RoqD3s_mQQ3UFx5GEwr1zcI3RVV9How_eJN8DAdanIgN_oW5plV__fYiCFAG9caztpoCbWIUUg4I3DWhzKTALacp5Mfbbv1DaZq-Dymxv_-BqCgtQsxYwE-7_esBo6i-vDUvnxKlVXssmmnY5UxLNkRznFbwCm40HbvfpzxphlDzybh1JnMQDabU1hIEwA4mfrm4FB0Xu7FDWsGT04pR4gygmEo1WKoQ-2wr3TQdOMXWT501WVl13cO85pzUJLXnDZIFub6VEU6qPKdeSc7TlOFsxJXjZWQLd4SNX2310wUR8TzJmtolauzlga0_oGGMTlNkTkc6aTG6kGK23ckLa76LlzQ_gpjFtrWVqjUJR-dWDtfNKZJ9YPutkLyOMK7CIGy6Jyub7BcTkJPGKuwxFr3gfvBBjWctAtqPUmm7Telh0kjzupOVcaUeWf59StuPDwljhPv2khWG-abasgr3n8LUHGMH6MDpH8asXntT.uTnDpuCwwehNhqvXCDJaUg/__results___files/__results___48_1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYDDLQdcqBWt"
      },
      "source": [
        "## Xception Net\n",
        "\n",
        "Esta arquitectura se basa en redes anchas y profundas deepwise. Estas redes tienen un amplio uso debido a dos razones.\n",
        "\n",
        "* Tienen menos parametros a ajustar comparado con otras arquitecturas. Lo que reduce el sobreajuste.\n",
        "\n",
        "* Son baratas computacionalmente, lo cual las hace favoritas en aplciaciones moviles.\n",
        "\n",
        "Estas redes se diferencian en dos procesos:\n",
        "\n",
        "1. Convoluciones anchas, sobre profundas\n",
        "\n",
        "2. Las convoluciones son punto a punto.\n",
        "\n",
        "Las convoluciones anchas se caracterizan en aplicar a un solo canal al tiempo operaciones de convoculción a diferencia de las tradiconales que recorren todos los canales. \n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*yG6z6ESzsRW-9q5F_neOsg.png)\n",
        "\n",
        "En la imagen cada kernel es de tamaño 5x5x1 e itera un canal de la imagen. Obteniendo productos escalares cada grupo de 25 pixeles. Obteniendo como salida una imagen de 8x8x3 \n",
        "\n",
        "Las convoluciones punto a punto se caracterizan por el uso de un kernel de tamaño 1x1 o un kernel que opera pixel por pixel. Este kernel tiene una profundidad igual a los canales de la imagen. \n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*37sVdBZZ9VK50pcAklh8AQ.png)\n",
        "\n",
        "Combinando estas dos dos operaciones se obtiene la arquitectua deepwise\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*VvBTMkVRus6bWOqrK1SlLQ.png)\n",
        "\n",
        "La diferencia puntal que maneja Xception se concentra en modificar esta arquitectura de la siguiente manera:\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*J8dborzVBRBupJfvR7YhuA.png)\n",
        "\n",
        "Esta arquitectura fue propuesta por Google.\n",
        "\n",
        "El sumario completo es el siguiente:\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*hOcAEj9QzqgBXcwUzmEvSg.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoE-SyaFp7BB"
      },
      "source": [
        "xception_model = applications.xception.Xception(weights = \"imagenet\", include_top=False, input_shape = (150, 150, 3))\n",
        "\n",
        "xception_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT5t3CVPxg3l"
      },
      "source": [
        "for layer in xception_model.layers[:5]:\n",
        "    layer.trainable = False\n",
        "\n",
        "#Agregando capas customizadas\n",
        "x = xception_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(1024,activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1024, activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "# creando el modelo final \n",
        "model_final =  Model(inputs=xception_model.input, outputs=predictions)\n",
        "\n",
        "model_final.compile(loss='binary_crossentropy',optimizer=optimizers.adam(),metrics=['acc'])\n",
        "\n",
        "history=model_final.fit_generator(train_generator,steps_per_epoch=200,epochs=1,validation_data=validation_generator,validation_steps=150)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSDKE4S0xu7q"
      },
      "source": [
        "## MobilNet\n",
        "\n",
        "Esta es la arquitectura preferida para aplucaciones mobiles, nuevamente fue propuesta por Google. Es una variación de Xception.\n",
        "\n",
        "Esta es la arquitectura\n",
        "\n",
        "![](https://www.researchgate.net/publication/327134257/figure/fig2/AS:661949444530178@1534832454962/Original-architecture-of-MobileNet-as-shown-in-the-MobileNet-paper-The-architecture-is.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDDNtiTZyVfX"
      },
      "source": [
        "mobile_model = applications.mobilenet.MobileNet(weights = \"imagenet\", include_top=False, input_shape = (150, 150, 3))\n",
        "\n",
        "for layer in mobile_model.layers[:5]:\n",
        "    layer.trainable = False\n",
        "\n",
        "#Agregando capas personalizadas\n",
        "x = mobile_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1024, activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "# creando el modelo final\n",
        "model_final =  Model(inputs=mobile_model.input, outputs=predictions)\n",
        "model_final.compile(loss='binary_crossentropy',optimizer=optimizers.adam(),metrics=['acc'])\n",
        "\n",
        "history=model_final.fit_generator(train_generator,steps_per_epoch=100,epochs=5,validation_data=validation_generator,validation_steps=50)\n",
        "\n",
        "acc=history.history['acc']  ##getting  accuracy of each epochs\n",
        "epochs_=range(0,epochs)    \n",
        "plt.plot(epochs_,acc,label='training accuracy')\n",
        "plt.xlabel('no of epochs')\n",
        "plt.ylabel('accuracy')\n",
        "\n",
        "acc_val=history.history['val_acc']  ##getting validation accuracy of each epochs\n",
        "plt.scatter(epochs_,acc_val,label=\"validation accuracy\")\n",
        "plt.title(\"no of epochs vs accuracy\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvwX2n9kynX2"
      },
      "source": [
        "![](https://www.kaggleusercontent.com/kf/16679264/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..zJeYgm3KlWgVxWherBudEw.G-7EjJ6IAoynRIEQYDeEi-mU0mQuUs7TDRR4lnpasnD7zyf-0RMUBIHoND96yXBQAAoTKBeuMj8_RHtWxJyJvwxssFKMmOyS6TBi4vs3pzvMGWqscAWOz2N9GEhOLcNvampUb2w26qQV8pXTT1zXyMq3sLu_jbW1yaPOWPK9u9MGJc7QR-TURW89ZvNwtPhsWZHxr3wlt-LgbRgBxPRame-_RoqD3s_mQQ3UFx5GEwr1zcI3RVV9How_eJN8DAdanIgN_oW5plV__fYiCFAG9caztpoCbWIUUg4I3DWhzKTALacp5Mfbbv1DaZq-Dymxv_-BqCgtQsxYwE-7_esBo6i-vDUvnxKlVXssmmnY5UxLNkRznFbwCm40HbvfpzxphlDzybh1JnMQDabU1hIEwA4mfrm4FB0Xu7FDWsGT04pR4gygmEo1WKoQ-2wr3TQdOMXWT501WVl13cO85pzUJLXnDZIFub6VEU6qPKdeSc7TlOFsxJXjZWQLd4SNX2310wUR8TzJmtolauzlga0_oGGMTlNkTkc6aTG6kGK23ckLa76LlzQ_gpjFtrWVqjUJR-dWDtfNKZJ9YPutkLyOMK7CIGy6Jyub7BcTkJPGKuwxFr3gfvBBjWctAtqPUmm7Telh0kjzupOVcaUeWf59StuPDwljhPv2khWG-abasgr3n8LUHGMH6MDpH8asXntT.uTnDpuCwwehNhqvXCDJaUg/__results___files/__results___61_1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJEiy86bysRE"
      },
      "source": [
        "## Resnet\n",
        "\n",
        "El mayor problema que debe enfrentar una red neuronal profunda es el gradiente desvaneciente, al incrementar la profundidad el valor de gradiente tiende a cero.\n",
        "\n",
        "Resnet fue una propuesta para lidiar con este problema, consiste en un modulo reisudal conectado en paralelo a la red neuronal que puede realizar el conjunto de intrucciones de su codigo o no.\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/750/1*ByrVJspW-TefwlH7OLxNkg.png)\n",
        "\n",
        "La arquitectura es la siguiente:\n",
        "\n",
        "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/08/08131926/temp12.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BtM5aL4yqiG"
      },
      "source": [
        "resnet_model = applications.resnet50.ResNet50(weights = \"imagenet\", include_top=False, input_shape = (150, 150, 3))\n",
        "\n",
        "for layer in resnet_model.layers[:5]:\n",
        "    layer.trainable = False\n",
        "\n",
        "#Agregando capas personalizadas\n",
        "x = resnet_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1024, activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "# creando el modelo final\n",
        "model_final =  Model(inputs=resnet_model.input, outputs=predictions)\n",
        "model_final.compile(loss='binary_crossentropy',optimizer=optimizers.adam(),metrics=['acc'])\n",
        "\n",
        "history=model_final.fit_generator(train_generator,steps_per_epoch=100,epochs=5,validation_data=validation_generator,validation_steps=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWOn15Iq2i9o"
      },
      "source": [
        "acc=history.history['acc']  ##getting  accuracy of each epochs\n",
        "epochs_=range(0,epochs)    \n",
        "plt.plot(epochs_,acc,label='training accuracy')\n",
        "plt.xlabel('no of epochs')\n",
        "plt.ylabel('accuracy')\n",
        "\n",
        "acc_val=history.history['val_acc']  ##getting validation accuracy of each epochs\n",
        "plt.scatter(epochs_,acc_val,label=\"validation accuracy\")\n",
        "plt.title(\"no of epochs vs accuracy\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLVkF7Lc2mCd"
      },
      "source": [
        "![](https://www.kaggleusercontent.com/kf/16679264/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..zJeYgm3KlWgVxWherBudEw.G-7EjJ6IAoynRIEQYDeEi-mU0mQuUs7TDRR4lnpasnD7zyf-0RMUBIHoND96yXBQAAoTKBeuMj8_RHtWxJyJvwxssFKMmOyS6TBi4vs3pzvMGWqscAWOz2N9GEhOLcNvampUb2w26qQV8pXTT1zXyMq3sLu_jbW1yaPOWPK9u9MGJc7QR-TURW89ZvNwtPhsWZHxr3wlt-LgbRgBxPRame-_RoqD3s_mQQ3UFx5GEwr1zcI3RVV9How_eJN8DAdanIgN_oW5plV__fYiCFAG9caztpoCbWIUUg4I3DWhzKTALacp5Mfbbv1DaZq-Dymxv_-BqCgtQsxYwE-7_esBo6i-vDUvnxKlVXssmmnY5UxLNkRznFbwCm40HbvfpzxphlDzybh1JnMQDabU1hIEwA4mfrm4FB0Xu7FDWsGT04pR4gygmEo1WKoQ-2wr3TQdOMXWT501WVl13cO85pzUJLXnDZIFub6VEU6qPKdeSc7TlOFsxJXjZWQLd4SNX2310wUR8TzJmtolauzlga0_oGGMTlNkTkc6aTG6kGK23ckLa76LlzQ_gpjFtrWVqjUJR-dWDtfNKZJ9YPutkLyOMK7CIGy6Jyub7BcTkJPGKuwxFr3gfvBBjWctAtqPUmm7Telh0kjzupOVcaUeWf59StuPDwljhPv2khWG-abasgr3n8LUHGMH6MDpH8asXntT.uTnDpuCwwehNhqvXCDJaUg/__results___files/__results___68_1.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnSXM0eo2qHN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}